{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8870a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from gammapy.maps import Map\n",
    "from astropy.coordinates import SkyCoord #, Angle\n",
    "from gammapy.modeling import Fit, Parameter, Parameters#, Covariance\n",
    "from gammapy.datasets import MapDataset #, MapDatasetNuisance\n",
    "from gammapy.modeling.models import (\n",
    "    PowerLawSpectralModel,\n",
    "    SkyModel,\n",
    "    PointSpatialModel,\n",
    "    GaussianSpatialModel,\n",
    "    Models,\n",
    "    FoVBackgroundModel,\n",
    ")\n",
    "\n",
    "import yaml\n",
    "import sys\n",
    "import json\n",
    "\n",
    "sys.path.append(\n",
    "    \"/home/hpc/caph/mppi045h/3D_analysis/N_parameters_in_L/syserror_3d_bkgmodel/4-Fitting_nuisance_and_model_parameters\"\n",
    ")\n",
    "from my_dataset_maps_19 import MapDatasetNuisance\n",
    "from  my_fit_19 import Fit\n",
    "#definitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ebc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "CLI=argparse.ArgumentParser()\n",
    "CLI.add_argument( \"--rnd\" )  \n",
    "CLI.add_argument( \"--amplitude\" )  \n",
    "CLI.add_argument( \"--false_est\" )   \n",
    "\n",
    "\n",
    "args = CLI.parse_args()\n",
    "input_ = dict()\n",
    "input_['rnd'] = int(args.rnd)\n",
    "input_['amplitude'] = float(args.amplitude)\n",
    "input_['false_est'] = bool(args.false_est)\n",
    "\n",
    "\n",
    "rnd = input_['rnd']\n",
    "amplitude =input_['amplitude']* u.Unit('cm-2 s-1 TeV-1')\n",
    "false_est = input_['false_est']\n",
    "\n",
    "print(\"....\" * 20)\n",
    "print(\"....\" * 20)\n",
    "print(\"....\" * 20)\n",
    "print(\"RND.................\", input_['rnd'])\n",
    "print(\"amplitude...........\", amplitude)\n",
    "print(\"false_est...........\", false_est)\n",
    "\n",
    "print(\"....\" * 20)\n",
    "print(\"....\" * 20)\n",
    "print(\"....\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f66d89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'amplitude' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20861/3352190535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mspatial_model_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pointsource_center\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutputfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/OOutput'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamplitude\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moutputfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'amplitude' is not defined"
     ]
    }
   ],
   "source": [
    "pos_frozen = True\n",
    "spatial_model_type = \"pointsource_center\"\n",
    "outputfolder = '/home/hpc/caph/mppi045h/3D_analysis/N_parameters_in_L/nuisance_summary/Robustness/output/data_asimov_tests'\n",
    "\n",
    "\n",
    "\n",
    "if spatial_model_type == \"pointsource_center\":\n",
    "    outputfile = '/home/hpc/caph/mppi045h/3D_analysis/N_parameters_in_L/nuisance_summary/Robustness/OOutput'+str(amplitude.value)+'.json'\n",
    "    \n",
    "with open(outputfolder+outputfile, 'r') as f:\n",
    "    data = json.load(f)\n",
    "j = 0\n",
    "rnds = list(data.keys()) \n",
    "\n",
    "print(\"setting to started ..\")\n",
    "print(\"fitting rnd dataset number:\", rnd)\n",
    "data[str(rnd)]['started'] = True\n",
    "\n",
    "with open(outputfolder+outputfile, 'w') as fp:\n",
    "    json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920cca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nui_distribution(nui_values_co, sys, mus, stds):\n",
    "    ii = i_end - i_start\n",
    "    ii = ii // 3 + 1\n",
    "    fig, axs = plt.subplots(ii,3, figsize =(10,ii* 3) )\n",
    "    for i,e in enumerate(xaxis[i_start:i_end]):\n",
    "        ax = axs.flatten()[i]\n",
    "        mu, sigma_ = mus[i+i_start], stds[i+i_start]  # mean and standard deviation\n",
    "        s = nui_values_co[i*amount_free_par : (i+1)* amount_free_par]\n",
    "        count, bins, ignored = ax.hist(s, 20, density=False, alpha = 0.3, color = 'red',)\n",
    "        #ax.set_xlim(-10,10)\n",
    "        ax.set_title(f'Energy: {xaxis[i+i_start].value:.2} TeV')\n",
    "        ax.plot(bins,   max(count) *\n",
    "                                np.exp( - (bins - mu)**2 / (2 * sigma_**2) ),\n",
    "                          linewidth=2, color='r')\n",
    "        ylim = ax.get_ylim()\n",
    "        \n",
    "        ax.vlines(0 +sys[i+i_start] *100, ylim[0], ylim[1],color = 'red' )\n",
    "        ax.vlines(0 -sys[i+i_start] *100 , ylim[0], ylim[1],color = 'red')\n",
    "    plt.tight_layout()\n",
    "        \n",
    "def plot_corr_matrix(dataset):\n",
    "    M = np.linalg.inv(dataset.inv_corr_matrix)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(M)  # interpolation='nearest')\n",
    "    fig.colorbar(cax);\n",
    "    print(\"Maximal expected sys amplitude in % of bg:\", np.sqrt( M.max() ) * 100)\n",
    "\n",
    "def plot_residual(dataset, max_ = None):\n",
    "    res_standard = (\n",
    "        dataset.residuals(\"diff/sqrt(model)\")\n",
    "        #.slice_by_idx(dict(energy=slice(6, 9)))\n",
    "        .smooth(0.1 * u.deg)\n",
    "        )\n",
    "    if max_ is None:\n",
    "        vmax = np.nanmax(np.abs(res_standard.data))\n",
    "    else:\n",
    "        vmax = max_\n",
    "    res_standard.slice_by_idx(dict(energy=slice(i_start,i_end))).plot_grid(add_cbar=1, \n",
    "                                                                  vmax=vmax, vmin=-vmax, cmap=\"coolwarm\")\n",
    "    return res_standard            \n",
    "\n",
    "def compute_K_matrix(l_deg, sigma, ndim_spatial_nui, ndim_spectral_nui,geom_down ):\n",
    "    helper_map = Map.from_geom(geom_down).slice_by_idx(dict(energy=slice(0, 1)))\n",
    "    helper_map2 = helper_map.copy()\n",
    "    ndim_spatial_nui_1D = int(np.sqrt(ndim_spatial_nui))\n",
    "    corr_matrix_spatial = np.identity(ndim_spatial_nui)\n",
    "    for b_0 in range(ndim_spatial_nui_1D):\n",
    "        for l_0 in range(ndim_spatial_nui_1D):\n",
    "            i = b_0 * ndim_spatial_nui_1D + l_0\n",
    "            C = SkyCoord(\n",
    "                helper_map.geom.pix_to_coord((l_0, b_0, 0))[0],\n",
    "                helper_map.geom.pix_to_coord((l_0, b_0, 0))[1],\n",
    "                frame=geom_down.frame,\n",
    "            )\n",
    "            helper_map.data[0, :, :] = C.separation(\n",
    "                geom_down.to_image().get_coord().skycoord\n",
    "            ).value\n",
    "            helper_map2.data = np.zeros(ndim_spatial_nui_1D ** 2).reshape(\n",
    "                helper_map2.geom.data_shape\n",
    "            )\n",
    "            helper_map2.data[0, :, :] = np.exp(\n",
    "                -0.5 * helper_map.data[0, :, :] ** 2 / l_deg ** 2\n",
    "            )\n",
    "            corr_matrix_spatial[i, :] = helper_map2.data.flatten()\n",
    "\n",
    "    corr_matrix_spectral = np.identity(ndim_spectral_nui)\n",
    "    for e in range((ndim_spectral_nui)):\n",
    "        corr_matrix_spectral[e, e] = sigma[e] ** 2\n",
    "    return np.kron(corr_matrix_spectral, corr_matrix_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd84e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/hpc/caph/mppi045h/3D_analysis/N_parameters_in_L/nuisance_summary/Robustness/0_estimate_sys_per_ebin.yml', \"r\") as ymlfile:\n",
    "    sys_read = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "mus = sys_read['mus']\n",
    "stds = sys_read['stds']\n",
    "\n",
    "path_local_repo = '/home/saturn/caph/mppi045h/Nuisance_Asimov_Datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mock_dataset(ad ):\n",
    "    added = \"_\" + str(ad)\n",
    "    dataset_N_sys = MapDatasetNuisance.read(f'{path_local_repo}/nui_dataset{added}.fits')\n",
    "    with open(f'{path_local_repo}/nui_par{added}.yml', \"r\") as ymlfile:\n",
    "        nui_par = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "    dataset_N_sys.N_parameters = Parameters.from_dict(nui_par )\n",
    "    bkg_model = FoVBackgroundModel(dataset_name=dataset_N_sys.name)\n",
    "    models = Models([])\n",
    "    models.append(bkg_model)\n",
    "    dataset_N_sys.models =models\n",
    "    return dataset_N_sys\n",
    "\n",
    "dataset_N_sys_ex = read_mock_dataset(0)\n",
    "emask = dataset_N_sys_ex.nuisance_mask.data.sum(axis=2).sum(axis=1) >0\n",
    "xaxis = dataset_N_sys_ex.geoms['geom'].axes[0].center\n",
    "i_start = 6\n",
    "i_end = i_start + sum(emask)\n",
    "\n",
    "downsampling_factor = 10\n",
    "ndim_spatial_nui = dataset_N_sys_ex.geoms['geom_down'].data_shape[1] **2\n",
    "print(\"# of Nuis per ebin:\", ndim_spatial_nui)\n",
    "l_corr, ndim_spectral_nui = 0.08,  i_end -i_start \n",
    "print(\"# of Ebins with Nuis:\", ndim_spectral_nui)\n",
    "bg = dataset_N_sys_ex.background\n",
    "bg_e = bg.data.sum(axis=2).sum(axis=1)\n",
    "amount_free_par = ndim_spatial_nui\n",
    "\n",
    "models = Models.read(\"1a-Source_models.yaml\")\n",
    "model_asimov = models[spatial_model_type]\n",
    "model_asimov.parameters['amplitude'].value = amplitude.value\n",
    "print(model_asimov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_fitting(dataset, mus, stds, falseningfactor):\n",
    "    Nuisance_parameters_correct = Parameters([Parameter(name = par.name, value =0 ,frozen = par.frozen)  \n",
    "      for  par in dataset.N_parameters])\n",
    "\n",
    "    dataset_fitting = MapDatasetNuisance(\n",
    "        background=dataset.background,\n",
    "        exposure=dataset.exposure,\n",
    "        psf=dataset.psf,\n",
    "        edisp=dataset.edisp,\n",
    "        mask_fit=dataset.mask_fit,\n",
    "        mask_safe=dataset.mask_safe,\n",
    "        inv_corr_matrix=dataset.inv_corr_matrix,\n",
    "        N_parameters=Nuisance_parameters_correct,\n",
    "        nuisance_mask=dataset.nuisance_mask,\n",
    "    )\n",
    "\n",
    "    bkg_model = FoVBackgroundModel(dataset_name=dataset_fitting.name)\n",
    "    bkg_model.parameters[\"tilt\"].frozen = False\n",
    "    models = Models(model_asimov) \n",
    "    models.append(bkg_model)\n",
    "    dataset_fitting.models = models\n",
    "    \n",
    "    if pos_frozen:\n",
    "        dataset_fitting.models.parameters['lon_0'].frozen = True\n",
    "        dataset_fitting.models.parameters['lat_0'].frozen = True\n",
    "    \n",
    "    ## Add systematic \n",
    "    sys_map = dataset.N_map().copy()\n",
    "    for e in range(24):\n",
    "        ex = dataset.exposure\n",
    "        ex_ = ex.slice_by_idx(dict(energy_true= slice(e, e+1)))\n",
    "        ex_.data = ex_.data / np.max(ex_.data)\n",
    "        sys_map.slice_by_idx(dict(energy= slice(e, e+1))).data *= ex_.data\n",
    "    sys_map.plot_grid(add_cbar = 1)\n",
    "    dataset_fitting.counts = Map.from_geom(dataset_fitting.geoms['geom'])\n",
    "    dataset_fitting.counts.data =  dataset.background.data * (1+sys_map.data) \n",
    "    \n",
    "    ## Add Source\n",
    "    dataset_fitting.counts.data += dataset_fitting.npred_signal()\n",
    "    \n",
    "    sys = (np.abs(mus) + falseningfactor* np.array(stds)) /100\n",
    "    print(sys)\n",
    "    correlation_matrix_co = compute_K_matrix(l_corr, np.array(sys[i_start:i_end]), \n",
    "                                             ndim_spatial_nui,\n",
    "                                             ndim_spectral_nui, \n",
    "                                      dataset_N_sys_ex.geoms['geom_down'])\n",
    "    dataset_fitting.inv_corr_matrix=np.linalg.inv(correlation_matrix_co)\n",
    "    \n",
    "    return dataset_fitting\n",
    "\n",
    "dataset_N_sys = read_mock_dataset(rnd)\n",
    "dataset_A_fitting = create_dataset_fitting (dataset_N_sys, mus, stds, 1)\n",
    "\n",
    "plot_residual(dataset_A_fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting Standard\" )\n",
    "# \"semi standard\"\n",
    "fit_N = Fit(store_trace=False)\n",
    "with dataset_A_fitting.N_parameters.restore_status():\n",
    "    dataset_A_fitting.N_parameters.freeze_all()\n",
    "    result_standard = fit_N.run([dataset_A_fitting])\n",
    "\n",
    "L_statsum_standard= dataset_A_fitting.stat_sum()\n",
    "print(f\"best fit {L_statsum_standard}\")\n",
    "with dataset_A_fitting.models.parameters.restore_status():\n",
    "    dataset_A_fitting.models.parameters['amplitude'].value = 0 \n",
    "    L_statsum_0_standard = dataset_A_fitting.stat_sum()\n",
    "    TS_standard =   L_statsum_0_standard - L_statsum_standard\n",
    "print(f\"0        {L_statsum_0_standard}\")\n",
    "print(f\"TS: {TS_standard}\")\n",
    "best_fit_model_standard = dataset_A_fitting.models[0].copy()\n",
    "best_fit_bgmodel_standard = dataset_A_fitting.models[1].copy(name='bestfit')\n",
    "\n",
    "print(best_fit_model_standard.parameters.to_table())\n",
    "print(best_fit_bgmodel_standard.parameters.to_table())\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e461e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting Corr Est.\" )\n",
    "fit_N = Fit(store_trace=False)\n",
    "#dataset_A_fitting.N_parameters.freeze_all()\n",
    "#for i in [145,146,147]:\n",
    "#    dataset_A_fitting.N_parameters[i].frozen = False\n",
    "result_N = fit_N.run([dataset_A_fitting])\n",
    "L_statsum_N= dataset_A_fitting.stat_sum()\n",
    "print(f\"best fit {L_statsum_N}\")\n",
    "with dataset_A_fitting.models.parameters.restore_status():\n",
    "    dataset_A_fitting.models.parameters['amplitude'].value = 0 \n",
    "    L_statsum_0_N = dataset_A_fitting.stat_sum()\n",
    "    TS_N =   L_statsum_0_N - L_statsum_N\n",
    "print(f\"0        {L_statsum_0_N}\")\n",
    "print(f\"TS: {TS_N}\")\n",
    "\n",
    "best_fit_model_N = dataset_A_fitting.models[0].copy()\n",
    "best_fit_bgmodel_N = dataset_A_fitting.models[1].copy(name='bestfit')\n",
    "\n",
    "print(best_fit_model_N.parameters.to_table())\n",
    "print(best_fit_bgmodel_N.parameters.to_table())\n",
    "print()\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbeeb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_nuis = dict()\n",
    "dict_nuis['corr'] = dataset_A_fitting.N_parameters.to_dict()\n",
    "\n",
    "with open(f'{path_local_repo}/OOutput{amplitude.value}/nui_par_{rnd}.yml', 'w') as outfile:\n",
    "        yaml.dump(dict_nuis, outfile, default_flow_style=False)\n",
    "\n",
    "        \n",
    "import json\n",
    "\n",
    "with open(outputfolder+outputfile, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "result = dict()\n",
    "\n",
    "\n",
    "result[\"L_statsum_N\"] = L_statsum_N\n",
    "result[\"L_statsum_0_N\"] = L_statsum_0_N\n",
    "result[\"TS_N\"] =  TS_N\n",
    "\n",
    "result[\"L_statsum_standard\"] = L_statsum_standard\n",
    "result[\"L_statsum_0_standard\"] = L_statsum_0_standard\n",
    "result[\"TS_standard\"] =  TS_standard\n",
    "\n",
    "\n",
    "result[\"success_standard\"] = result_standard.success\n",
    "result[\"success_N\"] =result_N.success\n",
    "\n",
    "\n",
    "par_names = best_fit_model_standard.parameters.names\n",
    "for par_name in par_names:\n",
    "    result[\"best_fit_\"+par_name+\"_standard\"] = best_fit_model_standard.parameters[par_name].value\n",
    "    result[\"best_fit_\"+par_name+\"_N\"] = best_fit_model_N.parameters[par_name].value\n",
    "\n",
    "    result[\"best_fit_\"+par_name+\"_error_standard\"] = best_fit_model_standard.parameters[par_name].error\n",
    "    result[\"best_fit_\"+par_name+\"_error_N\"] = best_fit_model_N.parameters[par_name].error\n",
    "\n",
    "    \n",
    "par_names = best_fit_bgmodel_standard.parameters.names\n",
    "for par_name in par_names:\n",
    "    result[\"best_fit_\"+par_name+\"_standard\"] = best_fit_bgmodel_standard.parameters[par_name].value\n",
    "    result[\"best_fit_\"+par_name+\"_N\"] = best_fit_bgmodel_N.parameters[par_name].value\n",
    "\n",
    "    result[\"best_fit_\"+par_name+\"_error_standard\"] = best_fit_bgmodel_standard.parameters[par_name].error\n",
    "    result[\"best_fit_\"+par_name+\"_error_N\"] = best_fit_bgmodel_N.parameters[par_name].error\n",
    "\n",
    "\n",
    "data[str(rnd)]['result'] = result\n",
    "with open(outputfolder+outputfile, 'w') as fp:\n",
    "    json.dump(data, fp, indent=4)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f37e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "    \n",
    "if false_est:\n",
    "    dataset_A_fitting_under = create_dataset_fitting (dataset_N_sys, mus, stds, 0.5)\n",
    "    dataset_A_fitting_over = create_dataset_fitting (dataset_N_sys, mus, stds, 2)\n",
    "    print(\"Fitting Under Est.\" )\n",
    "    fit_N = Fit(store_trace=False)\n",
    "    #dataset_A_fitting_under.N_parameters.freeze_all()\n",
    "    #for i in [145,146,147]:\n",
    "    #    dataset_A_fitting_under.N_parameters[i].frozen = False\n",
    "    result_N_under = fit_N.run([dataset_A_fitting_under])\n",
    "    L_statsum_N_under= dataset_A_fitting_under.stat_sum()\n",
    "    print(f\"best fit {L_statsum_N_under}\")\n",
    "    with dataset_A_fitting_under.models.parameters.restore_status():\n",
    "        dataset_A_fitting_under.models.parameters['amplitude'].value = 0 \n",
    "        L_statsum_0_N_under = dataset_A_fitting_under.stat_sum()\n",
    "        TS_N_under =   L_statsum_0_N_under - L_statsum_N_under\n",
    "    print(f\"0        {L_statsum_0_N_under}\")\n",
    "    print(f\"TS: {TS_N_under}\")\n",
    "\n",
    "    best_fit_model_N_under = dataset_A_fitting_under.models[0].copy()\n",
    "    best_fit_bgmodel_N_under = dataset_A_fitting_under.models[1].copy(name='bestfit')\n",
    "\n",
    "    print(best_fit_model_N_under.parameters.to_table())\n",
    "    print(best_fit_bgmodel_N_under.parameters.to_table())\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Fitting Over Est.\" )\n",
    "    fit_N = Fit(store_trace=False)\n",
    "    #dataset_A_fitting_over.N_parameters.freeze_all()\n",
    "    #for i in [145,146,147]:\n",
    "    #    dataset_A_fitting_over.N_parameters[i].frozen = False\n",
    "    result_N_over = fit_N.run([dataset_A_fitting_over])\n",
    "    L_statsum_N_over= dataset_A_fitting_over.stat_sum()\n",
    "    print(f\"best fit {L_statsum_N_over}\")\n",
    "    with dataset_A_fitting_over.models.parameters.restore_status():\n",
    "        dataset_A_fitting_over.models.parameters['amplitude'].value = 0 \n",
    "        L_statsum_0_N_over = dataset_A_fitting_over.stat_sum()\n",
    "        TS_N_over =   L_statsum_0_N_over - L_statsum_N_over\n",
    "    print(f\"0        {L_statsum_0_N_over}\")\n",
    "    print(f\"TS: {TS_N_over}\")\n",
    "\n",
    "    best_fit_model_N_over = dataset_A_fitting_over.models[0].copy()\n",
    "    best_fit_bgmodel_N_over = dataset_A_fitting_over.models[1].copy(name='bestfit')\n",
    "\n",
    "    print(best_fit_model_N_over.parameters.to_table())\n",
    "    print(best_fit_bgmodel_N_over.parameters.to_table())\n",
    "\n",
    "\n",
    "    dict_nuis['under'] = dataset_A_fitting_under.N_parameters.to_dict()\n",
    "    dict_nuis['over'] = dataset_A_fitting_over.N_parameters.to_dict()\n",
    "\n",
    "    with open(f'{path_local_repo}/OOutput{amplitude.value}/nui_par_{rnd}.yml', 'w') as outfile:\n",
    "            yaml.dump(dict_nuis, outfile, default_flow_style=False)\n",
    "\n",
    "\n",
    "    with open(outputfolder+outputfile, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    result[\"L_statsum_N_under\"] = L_statsum_N_under\n",
    "    result[\"L_statsum_0_N_under\"] = L_statsum_0_N_under\n",
    "    result[\"TS_N_under\"] =  TS_N_under\n",
    "\n",
    "    result[\"L_statsum_N_over\"] = L_statsum_N_over\n",
    "    result[\"L_statsum_0_N_over\"] = L_statsum_0_N_over\n",
    "    result[\"TS_N_over\"] =  TS_N_over\n",
    "\n",
    "    result[\"success_N_under\"] =result_N_under.success\n",
    "    result[\"success_N_over\"] =result_N_over.success\n",
    "\n",
    "\n",
    "    par_names = best_fit_model_standard.parameters.names\n",
    "    for par_name in par_names:\n",
    "        result[\"best_fit_\"+par_name+\"_N_under\"] = best_fit_model_N_under.parameters[par_name].value\n",
    "        result[\"best_fit_\"+par_name+\"_N_over\"] = best_fit_model_N_over.parameters[par_name].value\n",
    "\n",
    "\n",
    "        result[\"best_fit_\"+par_name+\"_error_N_under\"] = best_fit_model_N_under.parameters[par_name].error\n",
    "        result[\"best_fit_\"+par_name+\"_error_N_over\"] = best_fit_model_N_over.parameters[par_name].error\n",
    "\n",
    "\n",
    "    par_names = best_fit_bgmodel_standard.parameters.names\n",
    "    for par_name in par_names:\n",
    "        result[\"best_fit_\"+par_name+\"_N_under\"] = best_fit_bgmodel_N_under.parameters[par_name].value\n",
    "        result[\"best_fit_\"+par_name+\"_N_over\"] = best_fit_bgmodel_N_over.parameters[par_name].value\n",
    "\n",
    "\n",
    "        result[\"best_fit_\"+par_name+\"_error_N_under\"] = best_fit_bgmodel_N_under.parameters[par_name].error\n",
    "        result[\"best_fit_\"+par_name+\"_error_N_over\"] = best_fit_bgmodel_N_over.parameters[par_name].error\n",
    "\n",
    "\n",
    "    data[str(rnd)]['result'] = result\n",
    "    with open(outputfolder+outputfile, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
